[ { "title": "[Talk] Demystifying LLMs: How to Use GenAI Effectively (to keep your job)", "url": "/demystifying-llms/", "categories": "", "tags": "Talk, Software-Development-Process", "date": "2025-10-08 00:00:00 +0800", "snippet": "Abstract: Large Language Models (LLMs) like ChatGPT and Gemini are reshaping how people in every field approach problem-solving, from brainstorming ideas to debugging complex systems. This talk wi...", "content": "Abstract: Large Language Models (LLMs) like ChatGPT and Gemini are reshaping how people in every field approach problem-solving, from brainstorming ideas to debugging complex systems. This talk will explain, in straightforward terms, how LLMs work, what they’re good at, and where their boundaries lie.We’ll explore the principles of prompt engineering—how to frame questions and instructions to get clearer, more reliable results—and show practical ways to make LLMs a valuable part of everyday work. Attendees will leave with a grounded understanding of how to think about LLMs and concrete strategies for using them effectively in their own workflows. The talk was presented at: Tech Week Singapore 2025" }, { "title": "[Talk] You don’t need to hire a 10x engineer, you just need to enable the engineers you have", "url": "/no-10x-engineer-needed/", "categories": "", "tags": "Talk, Software-Development-Process", "date": "2024-11-17 00:00:00 +0800", "snippet": "Finally, after years of contractually required abstinence from public speaking, my new talk is out.Abstract: This talk discusses how a good developer experience (design system, scaffolding tools, ...", "content": "Finally, after years of contractually required abstinence from public speaking, my new talk is out.Abstract: This talk discusses how a good developer experience (design system, scaffolding tools, tech standards, …), combined with the right organisational structures and operating model, a lean team composition with a focus on autonomy, and an organisation that measures individual impact can lead to high performing teams with exceptional productivity. The talk was presented at the following events: API Days Singapore 2024 JuniorDevSG - Code &amp; Tell Data Architecture Singapore Tech Week / Cloud Expo Asia 2024 Stack 2024" }, { "title": "The most productive shell commands and command line tricks", "url": "/shell-commands/", "categories": "", "tags": "Command-line, Shell, Bash, Zsh", "date": "2021-04-04 00:00:00 +0800", "snippet": "When developing software, no matter what technology you are working with there is no way around using the command line when you want to be a productive developer.This a list of my favorite and most...", "content": "When developing software, no matter what technology you are working with there is no way around using the command line when you want to be a productive developer.This a list of my favorite and most used shell commands and tricks that I learned over the years. I’m sure you know some of them already so feel to skip the ones you know but others might bring you a productivity boost and lets you show off your 1337 h4x0r $killz in front of your n00b colleagues.Disclaimer: I use these commands on MacOS with Z Shell(Zsh) in Iterm2. As long as you are running a bash-like shell on a Unix &amp; -like OS these commands should work for you as well. If you have a more exotic set up, you probably know your way around the shell to make these work yourself. If you run Powershell on Windows: Good Luck!cd -You probably know that you can use cd to change into a certain directory.But did you know you can use the dash (-) as an argument to go back to the previous directory?$ cd /home~&gt; home$ cd /my_dir~&gt; /my_dir$ cd -~&gt; /homeBonus fact: The dash argument also works with git checkout, so you can e.g. quickly switch between master and a working branch.For a larger directory history check out pushd and popd.Shell HistoryProbably the most used shell trick I use is to press the up arrow (successively) to select the last commands of my shell history.Accompanied by ctrl + r (successively) to reverse search through my shell history by a keyword in LRU order.Or type history directly to see the whole shell history in your terminal. You can then write ![number] to select the command at position number in your history.You can even use a negative number as that index to select the k-th last command, like so:$ echo second$ echo last$ !-2-&gt; echo second!!You can also include !! in your command, and it gets substituted with the previously executed command.$ apt-get install unicorn-factory&gt; [...] Permission denied$ sudo !!&gt; sudo apt-get install unicorn-factory!:[index]Or you can select just parts of the last command by including !:[index] in your command and the word at the index [index] in the previous command gets inserted.$ echo hello world&gt; hello world$ echo !:1&gt; helloYou can even select ranges with [index]-[index] …$ echo live long and prosper&gt; live long and prosper$ echo !:3-4&gt; and prosper!^ !$There are also shortcuts for the first(!^), and the last(!$) parameters of the previous command.$ echo live long and prosper&gt; live long and prosper$ echo !^ !$&gt; live prosperEditing the current lineThis can be especially useful when you have just selected a command from the history which needs some slight change.You can move your cursor to the beginning of the line by pressing ctrl + a and to the end of it by pressing ctrl + e (remember: e for _e_nd and a for, erm … the beginning of the alphabet(?))Additionally: ctrl + w cuts the word to the left of the cursor alt + d cuts the word to the right of the cursor ctrl + k cuts everything to the right of the cursor ctrl + u cuts everything to the left of the cursor ctrl + y pastes back what you have just cutctrl + x + eIf you realize you actually need to make a bigger edit or write a longer command you can also switch to your editor and take current line with you.$ you are typing this really long command, maybe with some loop or some complex parsing logic and then realize you need more editing power so you press ctrl + e + xBOOOM!VIM(or Nano or VI etc.) opens with your command you had typed so far already in the bufferPaste modified command from historyInstead of retrieving the last command and then modifying it in two separate steps you can also do it in one step.^x^y gives you the previous command with x replaced by y$ gti status&gt; Command 'gti' not found,$ ^gti^git&gt; git statusHandling multiple files with one commandYou might have used commands like cp or mv before to handle files.One of my favorite shortcut is the {} parameter expansion.By using {} you instruct your shell to expand each value on the curly brackets.$ mv hello_world.{js,html} staticThis command moves both the hello_world.js file and the hello_world.html file without needing to type hello_world. twice.You can also use ranges…The following command moves 5 files (file1.png, file2.png, file3.png, file4.png, and file5.png) to the backup/ directory.$ mv file{1..5}.png backup/As there are many more commands that will help you be productive I will be constantly updating this list when I come across new jewels, so you might want to bookmark this article for future references.In the meantime share your favorite command in the comments." }, { "title": "[Talk] Artificial Intelligence? - more like Artificial Stupidity!", "url": "/artificial-stupidity/", "categories": "", "tags": "ML, Machine-Learning, AI, Artificial-Intelligence, Talk", "date": "2020-06-13 00:00:00 +0800", "snippet": "Abstract: Nowadays “Artificial Intelligence” is everywhere! And rightly so, it does enable us to do really cool things, things we couldn’t even imagine doing just a decade ago. In fact, it sometim...", "content": "Abstract: Nowadays “Artificial Intelligence” is everywhere! And rightly so, it does enable us to do really cool things, things we couldn’t even imagine doing just a decade ago. In fact, it sometimes just feels like magic. This ‘magic’ behind it is often powered by “Machine Learning”. But even “AI” has its limitations. I’ll show examples where “AI” and ML have failed (sometimes with horrible consequences) and will explain why failures are unavoidable in ML but also mention what we can do to reduce them in the future. Furthermore, I’ll showcase how current AI implementations discriminate against minorities and how that in some cases even leads to a higher risk of death for those groups. I’ll cover the bias that humans introduce and I’ll explain how poor choice of data makes our world even more unjust than it already is. The takeaway for the audience: AI can fail and sometimes it has horrible consequences. Why is AI so hard to “do right”? How can we make AI better?You can watch the talk by clicking any of links or the video below. The talk was presented at the following conferences: NDC Sydney 2020 Remote Chaos Communication Congress C# corner AMA (interview and presentation) Java Day Istanbul 2020 Light Up 2020 (Unicef fundraiser) Tech community day JAX 2020 DevTalks Romania Stackconf 2020 DataEngBytes Australia Global AI on tour (Belgium edition) Clearwater Development Conference 2020 AI DevWorld 2020 Update conference 2020 Stack 2020 API days Paris 2020 FOSSASIA Summit 2021" }, { "title": "Analysing IMDB TV series data aka How Wesley Crusher affects Star Trek episode ratings", "url": "/imdb-episode-ranking-star-trek/", "categories": "", "tags": "IMDB, Python, Star-Trek, Data", "date": "2020-02-10 00:00:00 +0800", "snippet": "Have you ever wondered how much influence a single actor has on the rating of a TV show’s episodes? This article will showcase you that you can analyse data from the internet movie database(IMDb) f...", "content": "Have you ever wondered how much influence a single actor has on the rating of a TV show’s episodes? This article will showcase you that you can analyse data from the internet movie database(IMDb) for this, using Star Trek The Next Generation as example series.Some time ago I found out that my partner had not yet watched Star Trek The Next Generation(or any Star Trek series for that mater 😱). I was excited for her! She was going to experience something that anyone of us can only experience once in a life time: watching TNG for the first time! I was also a little bit excited for myself since it meant I could watch TNG again!Some episodes into the first season I had noticed two things:First:Riker without a beard is just not the real Riker!And Second:I realized how annoyed I get when Wesley Crusher (played by Wil Wheaton) showed up.I brought this second topic up with my partner, and explained how I think he is just an arrogant little brat. Surprisingly, my partner disagreed with me! And according to her “Wesley is cute”!Now, even though I felt strongly about this, I know better than simply arguing back and forth about this matter (of taste?). Especially because I didn’t want to ruin TNG watching experience.Instead I turned to Science!Data Science to be exact.(well… It’s not really data science, I’m simply plotting some numbers and calculating some averages, please don’t take this post too serious.)After a short search I found IMDbPy, a python package to retrieve data from IMDb. I hacked in a couple of lines of code¹ and ta-da!I calculated the average episode rating of all episodes, as well as the average episode ratings for episodes with Wesely Crusher and without him. I also plotted all the episodes ratings for good measure.As a result: I could show my partner proof that episodes with Wesely Crusher’s appearance have on average a lower rating than episodes without him.To be fair, this does not mean that Wesely Crusher’s appearance makes an episode bad.There are a lot of other factors that influence this average episode rating. I already mentioned Rikers beard above, then there is S02E22 which (as seen in the plot) is an outlier related to the writer’s strike that year. And there are many other things related to this e.g. viewers building emotional bonds with the characters over the early seasons and so on.As usual: Correlation is not causation!Nevertheless, I ran the same code for some other non-permanent characters as well.For example Colm Meaney who plays Chief O’Brien.And Barclay who is played by Dwight Schultz who, surprisingly for me, correlates with a relatively high episode rating.I also extended the code even further to support comparisons between two non-permanent actors in the same plot.As an example you can see the plot for Dr. Crusher(Gates McFadden) and Dr. Pulaski(Diana Muldaur), which show a few interesting findings. The writer strike episode which I mentioned above is coincidentally the only episode where both show up. Unsurprisingly to most TNG fans (my partner excluded…🤷‍♂️), the average rating of episodes showing Dr. Pulaski is lower than the equivalent with Dr. Crusher. There are exactly two episodes without any of the doctors and the average rating of these two is higher than the average of either of the doctors alone. This can however mostly be attributed to the excellent rating of Episode S02E16 “Q Who”. Speaking of “Q”, if you have ever watched The Next Generation you know that one of the most likeable characters of the series is in fact Q, played by John de Lancie, he doesn’t appear often, but when he does you know you are in for a treat! Similar things can be said about the appearances of Whoopi Goldberg who played Guinan, the counseling bartender.There are exactly two episodes where both of them show up and those are two outstanding episodes! The graph below shows each episode rating plotted as a bar, with a blue bar representing an episode with Guinan, a red bar is used for an episode with Q, and a green bar representing both appearing in that episode!The average rating of the two episodes with actors reaches an incredible 8.8! John de Lancie’s appearances reach an impressive average of 8.0.The full source code can be found here.Next steps: There are a few next steps that can be done to get some more information out of this data: Look at episode description and find the general episode themes and see how they rank against each other. (eg. look for keywords like Borg, Cardassian, Worf etc. )find all non-permanent cast automatically¹: we all know that it’s not just a couple of lines of code, and it surely didn’t work on the first try but this is a blog post and you have no proof that I failed!" }, { "title": "The origin of Smoke Testing and the confusion it can cause", "url": "/smoke-testing-origin/", "categories": "", "tags": "Testing, Smoke-Testing", "date": "2019-08-05 00:00:00 +0800", "snippet": "You know what smoke testing is, right? Yes! Great, I also thought I knew, as did a colleague of mine. The problem: each of us had a different interpretation of the meaning.As mentioned in my articl...", "content": "You know what smoke testing is, right? Yes! Great, I also thought I knew, as did a colleague of mine. The problem: each of us had a different interpretation of the meaning.As mentioned in my article, visualization and prioritization of technical debt, analogies can significantly help with grasping abstract concepts. This is where a term like “Smoke Testing” can come in handy, as long as everyone involved has the same understanding of the analogy.The storyA colleague and I were discussing how to verify the functionality of our recent production deployment. He mentioned that we could do some manual smoke testing. This confused me a little bit since that’s not how I understood “smoke testing”.Luckily for me, I knew the origin of the analogy, and I could derive what ‘’smoke testing’’ in software development means. So I started to explain why smoke testing has this name, and why his understanding was wrong.Hardware Smoke TestingI pointed out that the term “smoke testing” originated from hardware development. If you power on a circuit board for the first time and you see smoke rising, you know it’s broken.Therefore, I was arguing, that simply booting up the application and making sure that the application doesn’t immediately die, would be a sufficient smoke test. This could be done automatically as a step in the build pipeline. “Lessons Learned in Software Testing” by Cem Kaner, James Bach, Brett Pettichord describes the origin as follows:\"The phrase smoke test comes from electronic hardware testing. You plug in a new board and turn on the power. If you see smoke coming from the board, turn off the power. You don’t have to do any more testing.\"Smoke Testing in PlumbingSo far so good, right? I felt my explanation was sound and since it matched any uses of the term I had seen in software development, I had a valid reason to believe that this was the one and only explanation of the term and my colleague could do nothing but agree with me.Boy, was I wrong!Right after I finished my explanation, my colleague (who might or might not have been a plumber in a previous life) started sharing his understanding of the origin of smoke testing and it was a very different story.In the plumbing industry, smoke testing is done to verify that a pipe is not leaking.Smoke is created artificially, blown into tubing, and then the pipes are examined to make sure there is no smoke leaking.Transferring this understanding to software development, therefore, meant that after the application start, multiple simple user flows are executed to ensure basic functionality. And he added, in his understanding, this was mostly done manually in contrast to a full and automated functional test suit.My mind was blown!Not only did my colleague has a different story to tell about the origin of the term smoke testing but it all made sense as well. And I googled it – plumbers do actually do that and it is indeed called smoke testing.So, where did we go from there?What we both agreed upon is that smoke testing: Tests basic functionality Is one of the first, if not the first test after deployment Provides fast feedback to see if further testing even makes senseHowever, we were indifferent about what basic functionality meant in this context. So we looked around for what other people had to say about “smoke testing”. We found multiple definitions and the lines between these definitions were foggy (pun intended).So we finally agreed on something that made sense to us in our situation and went with that for the duration of the project.Conclusion What smoke testing really means for you depends on your team. Whatever meaning you chose does not matter, just make sure that everyone on your team has the same understanding. When talking about it with people outside of your team be aware of different meanings and clarify first before it results in misunderstandings. Regardless of the terminology that you agree upon, smoke tests usually offer fast feedback to your development process and codebase. (This article has previously been published on ThoughtWorks.com)" }, { "title": "[Talk] Consumer Driven Software Development", "url": "/consumer-driven-software-development/", "categories": "", "tags": "Agile, Software-Development-Process, Talk", "date": "2019-06-25 00:00:00 +0800", "snippet": "Abstract: Not another one of these * Driven Development, you say? This one is different, I promise! Writing good Software is difficult! It is mentally exhausting! Programmers need to make decisio...", "content": "Abstract: Not another one of these * Driven Development, you say? This one is different, I promise! Writing good Software is difficult! It is mentally exhausting! Programmers need to make decisions all the time: Should I write a comment here? Does this function belong to another class? What should the test name be? In this talk I propose a mindset that let’s you make better, more reasonable decisions more easily. I’ll be using examples from daily development life situations that show the effectiveness of Consumer Driven Software Development and by itself are already worth applying. TL;DR: Write better software more easily with this easy trick! Software development coaches hate him! " }, { "title": "An ML showdown in search of the best tool (2019)", "url": "/ml-showdown-in-search-of-the-best-tool/", "categories": "", "tags": "AI, Artificial-Intelligence, ML, Machine-Learning", "date": "2019-03-10 00:00:00 +0800", "snippet": "Ever burgeoning digital data combined with impressive research has lead to a rising interest in Machine Learning or ML, which has further powered a vibrant ecosystem of technologies, frameworks, an...", "content": "Ever burgeoning digital data combined with impressive research has lead to a rising interest in Machine Learning or ML, which has further powered a vibrant ecosystem of technologies, frameworks, and libraries in the space. So, today, when technologists are trying to solve a problem leveraging ML, the sheer volume of possible approaches can leave them overwhelmed. This is exactly why I decided to lift the dense fog from around the current ML landscape by delving into this tech and its popular subset, deep learning.I have employed a data-driven approach and have created a ranking system where each ML tech is rated on certain features. This should help decide what works best for a technologist’s use case scenario. I also restricted my analysis to the most popular technologies that, at the time of writing this paper, enjoyed no less than a 6.500 stars (for deep learning) or 1.300 stars (for statistical learning and shallow neural networks) on Github.Here are the factors that I took into consideration for my data-driven analysis Ease of setup: A new technology can set the mood for the entire development journey which is why you need to weigh the difference between a simple pip/brew/conda/apt-get install, and building a tool and its dependencies from source. Usability: The learning curve coupled with the tool’s intuitiveness are good indicators of an ‘easy or tough application’. But, these are rather subjective factors because they are determined by one’s own level of experience as well. Adoption rate within the community: Popularity can hint at a tool doing something right. That popularity is also an indicator for the community support one can expect when running into problems while working with the said tool. Degrees of freedom: Some models’ parameters might need specificity while others allow for flexibility. An example of this is the number of possible infrastructure configurations. And, this could be a deterrent or not depending on one’s use.Now, let’s look at the devised ranking system based on the above-discussed parametersThis rating system will help one arrive at meta conclusions regarding the technology into consideration. In other words, it will help one make broad decisions regarding their development path, but the finer points will depend on the definition of the task laid out before them.The machine learning tech showdownBased on the qualifiers/factors discussed above, I have plotted technologies on a grid. Additionally, in spite of deep learning being a subset of ML, both enjoy different use cases, especially due to their GPU support. Which is also why I have split the two into separate categories. Let’s go over the grid’s coordinates - The y-axis shows increasing degrees of freedom The x-axis shows increasing degrees of adoption within the tech community Size of bubbles correlates with usability. The easier to use, the bigger the bubble.A few broad observations: Scikit-learn sees high adoption from the tech community. The most probable reason is a powerful Python interface that allows tweaking of models across multiple parameters. MLlib and H2O should be considered when working with Spark. Spark does come with MLlib and has a higher level wrapper called SparkML that supports the same. In some cases, H2O offers a suitable solution and can be used alongside MLlib or added when needed. Weka’s interface makes it the easiest to use but, it isn’t popular within the tech community. Interestingly, one should also note that Weka offers additional data mining features. PyBrain’s development has been discontinued apart from 10 small bug fixing commits over the last two and a half years.ML tools scalabilityFor those of you whose main concern is the tech/tool’s scalability, this perspective of the landscape could help - The y-axis shows increasing degrees of freedom The x-axis shows increasing degrees of the tech’s scalability Size of bubbles correlates to usability. The easier to use, the bigger the bubble. The color of the bubbles are indicative of the tech’s adoption within the communityA few broad observations: H2o, MLlib, and Mahout can be integrated with Spark and run on top of a HDFS cluster In spite of Scikit-learn offering both, the highest degree of freedom and adoption rate, it’s limitation is when working with large amounts of data. It is also limited in terms of scalability because of limited memory and resource capacity.The deep learning tech showdownIf deep neural networks are what one is looking for, scalability is never going to be a problem, because it’s a default feature of almost all deep learning tech.Most of the frameworks that I have analyzed in this section are open sourced. Companies like Google, Facebook, and Microsoft handle large amounts of data that reveal powerful insights, and it serves the parent companies well to have the larger tech community work on their respective frameworks and make improvements on them. The y-axis shows increasing degrees of freedom The x-axis shows increasing levels of adoption by the tech community Size of bubbles correlates to usability. The easier to use, the bigger the bubble. The color of the bubbles are indicative of how easy the tech is during setupA few broad observations: Tensorflow is the most popular framework, and contributing factors are the clean and comprehensive Python API, the multiple language wrappers and the several optimizations that are crucial to working with distributed infrastructure. Also, Google’s Tensorflow displays the maturity that comes from being in the market for more than two years. However, a negative is the steep learning curve needed to master the framework. Keras is not necessarily a deep learning framework, but a wrapper for several frameworks; a higher level interface if you will. Keras allows for the quick building of deep learning prototypes on top of frameworks and ensures an easy interface built on powerful optimizations. In fact, combining Tensorflow with Keras is quite popular. This is a key factor for the frameworks in the middle of the chart, that fight for attention. Caffe2 is a fairly new re-implementation of Caffe. And, I foresee Caffe2 joining the ‘attention seeking’ group soon especially since its Github repository got merged with the one of PyTorch. The original Caffe framework is still popular when it comes to image processing. University of Montreal’s Theano that has been around for a while recently lost support in the overcrowded market PyTorch, a re-implementation of Torch, is slowly set to replace Tensorflow in academic research. This is because the ML library is implementing the latest research results faster than its competitors. DL4J is, as the name suggests, a library for Java but has severe competition because most of the other frameworks also offer a Java interface Microsoft Cognitive Toolkit has a low rating when it comes to setup. For a fair comparison, I installed all the frameworks on Ubuntu Linux because not all (mentioned) frameworks support Windows. A Windows installation might probably be trouble-less in this case.Interestingly, the path to success is not always obvious when it comes to ML and deep learning tech. More often than not, experimentation or trial and error will identify which model in combination with what hyperparameter offer the best results. For example, deciding if a Bidirectional Recurrent Neural Network, BRNN performs better than a Long Short Term Memory Network, LSTM in one’s use case can only be determined by trying out different combinations of the above and comparing results, which points to the fact that the deciding factor for or against a tool is its ability to enable fast prototyping.There is also the approach of fine-tuning a model’s every little parameter that will obviously ensure more options available on hand, which in turn guarantee a great end result. It is therefore advisable that one is open to exploring different tools at different phases of the project.Finally, let’s look deep learning tech during a project’s life cycle.There is some work being done in the space of framework interoperability solutions. ONNX is a pertinent example. Short for Open Neural Network Exchange, it’s a unified format to define a neural network model. Currently Caffe2, Microsoft Cognitive Toolkit, MXNet, and PyTorch store their network description in the ONNX format. Additionally, there are other ways to convert one frameworks’ format to another.Facebook, which is responsible for two of the frameworks I have analyzed; Caffe2 and Pytorch, is making use of this interoperability feature. PyTorch is used for prototyping and research, and the model is then moved to Caffe2 during production. To make things easier, Facebook merged the GitHub repositories of both, and this is a first step in the direction of merging both the frameworks into one.Even though MxNet supports ONNX, it tries to address the need for switching frameworks by offering options at different parts of the product life cycle. It has a built-in higher-level interface, which allows for fast prototyping and fine-tuning.Lastly, but not least are the set of tools to the extreme left of the graph. These are various as-a-service solutions offered by cloud providers, and they offer specialized solutions like Optical Character Recognition, Landmark Detection, Face Detection, Text and Speech Translator, Speaker Recognition, etc. They don’t usually allow one to train a model and are more an API to upload files to and receive a JSON containing the analyzed data. Also, they are paid for offerings and one’s data is accessible by the parent companies.I’d like to reiterate that the graphs or diagrams that I have put together are a depiction of the current landscape based on what I have deemed meaningful criteria. The dynamic nature of technology requires that one use these ranking systems only as a starting point or a guideline, for their own analysis. I’d suggest one starts off with these factors; project lifecycle phase, scalability and community support, and depending on their use case they might want to include specific features to better arrive at the right choice for their particular requirement.(This article has originally been published on ThoughtWorks.com)" }, { "title": "Implementations of Artificial Intelligence in 2019 featured in FinTech News and Data Quest India", "url": "/implementations-of-artificial-intelligence-in-2019/", "categories": "", "tags": "AI, Artificial-Intelligence", "date": "2018-12-26 00:00:00 +0800", "snippet": "FinTech NewsandData Quest Indiafeatured me and my predictions for artificial intelligence in 2019.Check out there websites or read the quote here: 2018 can easily be called the ‘Year of the Chatbo...", "content": "FinTech NewsandData Quest Indiafeatured me and my predictions for artificial intelligence in 2019.Check out there websites or read the quote here: 2018 can easily be called the ‘Year of the Chatbot’. While most chatbots have been rudimentary FAQ replacements, we should start seeing further influences of natural language processing that will propel the chatbot evolution to further ‘seamless interactive’ heights. Now, broadening that overview, one can comfortably say that AI has matured quite quickly over the last few years and is easier to implement now, than ever before – thanks to computing and storage access, connected systems and the data deluge. For instance, 2019 will definitely see aggressive AI implementation in fields like healthcare and retail where extensive groundwork has been carried out over the recent past. Looking inward, AI, as the child of IT is an inherent part of the industry and will increasingly drive more aspects of the software development life cycle like application testing and cybersecurity, says Aiko Klostermann, Technology Consultant, ThoughtWorks." }, { "title": "[Talk] Machine Learning Landscape Today (2019)", "url": "/machine-learning-landscape-today/", "categories": "", "tags": "ML, Machine-Learning, Talk", "date": "2018-12-17 00:00:00 +0800", "snippet": "Abstract: When you have a data centric problem to solve and you look for a technology to support you with this: The machine intelligence landscape can be overwhelming. I analysed the landscape usi...", "content": "Abstract: When you have a data centric problem to solve and you look for a technology to support you with this: The machine intelligence landscape can be overwhelming. I analysed the landscape using a data driven approach and condensed the outcome into a consumable from. Additionally I came to the conclusion that there is a set of questions you have to ask yourself to make the best possible choice for your situation. This talk will cover these questions and present the analysis results on the basis of these. TL;DR: With this simple talk you will know which framework / toolkit / library is best suited for your problem. Machine Learning Consultants hate him!The talk was presented at the following conferences. Devoxx UA 2018 FOSSASIA Summit 2019 Great International Developer Summit 2019You can watch the talk by clicking the links above or the video below.You can also read the article that summarizes the talk here. " }, { "title": "Code Coverage - The metric that can make your tests worse", "url": "/code_coverage/", "categories": "", "tags": "Code-Coverage, Testing, Software-Development-Process", "date": "2018-12-01 00:00:00 +0800", "snippet": "There are a lot of issues with code coverage: Eloquently summed up by Stackoverflow.com user Mark Simpson: Code coverage tells you what you definitely haven’t tested, not what you have.Just like w...", "content": "There are a lot of issues with code coverage: Eloquently summed up by Stackoverflow.com user Mark Simpson: Code coverage tells you what you definitely haven’t tested, not what you have.Just like with other metrics, there are ways to misuse it. In this article, I will not list all the flaws with it (you can Google them later). Instead, I will focus on one practice that caught my attention and that I am encountering more and more lately: Setting a static code coverage target.Teams sometimes calculate code coverage during the build and add a check that fails it when coverage drops below a predetermined threshold. This is usually done to ensure that a newly written code is sufficiently tested.But what should this predetermined threshold be?Considering that not all code needs to be tested (Kent Beck agrees on that), having a threshold of 100% is not reasonable. Obvious examples for code that does not usually need to be tested are data transfer objects (DTOs) and automatically generated code.What a reasonable amount of code coverage is depends on your code and your application!How can setting a static code coverage threshold make my tests worse then?!Since you are reading this article about code coverage, I assume you know the importance of a reasonable test suite. Therefore you want to actually write all the necessary tests while developing.The problem occurs when a static code coverage threshold is set, independent of your specific code base, possibly even by someone outside the development team. The intentions are usually good. The threshold should encourage you to write tests.However, as mentioned above, what a reasonable code coverage is depends on your code. And your code base is different from some other teams. It also changes with every commit. Having a static threshold set by people outside the team is unlikely to be the exact threshold you would have come up with when reasonably approaching this.This can lead to two possible situations: Your coverage is above that static threshold.or Your coverage is below that static threshold.Let’s have a look at these two situations:Case 1: Your coverage is above the thresholdAt first sight, this seems like a great situation. Everybody is happy and the goal is achieved! What could possibly be wrong about this?Let’s a little deeper into this though:In this situation, when continuing the development, at best the developers simply ignore that threshold, they continue to write tests as usual and the whole threshold (and its integration into the build process) adds nothing to the development process.What it also does is it suggests that a reasonable amount of code coverage has been reached to the people who set this threshold. While the truth is that the threshold simply might be too low to be any kind of useful indicator and therefore giving a false sense of security.The worst impact this low threshold and it’s false sense of security has is the following:Developers simply stop writing tests because “The coverage is already high enough”.This is the exact opposite of what the people setting the threshold try to achieve with this undertaking.Case 2: Your coverage is below the ThresholdKeeping in mind that you know the importance of a reasonable test suite and actually want to write all the necessary tests. Let’s have a look at the second situation where the actual coverage is below that arbitrary threshold, even though you have written all the reasonable tests.I can think of two possible options you have to still reach the threshold. Excluding new code from the coverage report. Writing more tests.Option 1: Excluding code from the coverage report:This option is only applicable when you are in control over the coverage runner.Some code coverage runners allow the user to exclude certain parts of the code so they are not counted towards the overall coverage result.Many of these runners, however, only allow whole files/packages to be excluded. Given that a class might have code that needs to be tested and also code that does not need to be tested you may end up unintentionally excluding code that you actually should test. An example for this could be a class with custom logic and an auto-generated hashCode() function. The coverage report will, in this case, not call out when there is untested code in that file/package.This is again the exact opposite of what the person who set up the threshold tries to achieve.Option 2: Writing more tests:As mentioned above, I assume you value a sufficient test suit and you already wrote all the tests you considered reasonable. But when you now still need to increase your code coverage to reach that threshold, the only thing left is writing unreasonable tests.This simply is a waste of time, effort and money and surely not what the people who set up the threshold in the first place had in mind.At the end it boils down to Goodhart’s law, which states: When a measure becomes a target, it ceases to be a good measure.If you feel that not enough code is covered by tests, I’d suggest, rather than imposing (potentially) counterproductive measures like a static code coverage threshold, enable your team.Focus your effort on developer training about tests — you’ll get it back manifold when your colleagues understand the real benefits of a reasonable test suite.Summary: Code coverage threshold needs to be refined “organically”. The ideal amount of coverage depends on your code base and changes with every commit. If you have a static threshold instead: You might end up wasting time by writing useless tests, or stop writing tests that are actually necessary.Addendum: How I personally use code coverageThe way I personally use code coverage is by running a build script with a test suite and coverage report after I finished a story. After running it, I check the coverage report to see if I might have missed testing something. If this is the case I then go back to the code and decide whether a test needs to be written here or not.To be precise, I look at branch coverage as it can be more indicative of the comprehensiveness of a test suit compared to line coverage.To be honest, I trust computers’ memory more than my own memory, and that’s why my build script does, in fact, have a code coverage threshold that fails the build if I missed out on tests. So, even if I forget to check the coverage report and I did miss the threshold, I have a gentle reminder that I have to check if another test is necessary.The important part is that the threshold is not static!Since I am in control over the threshold I can adjust that threshold when I realize it’s failing the build but another test would be unreasonable.Additionally, I also adjust the threshold when my changes increase the overall coverage. This is so that I don’t miss a reminder on future commits when necessary.I try to keep the threshold as close as possible to the actual coverage." }, { "title": "Visualising and Prioritizing Technical Debt", "url": "/visualising-and-prioritizing-technical-debt/", "categories": "", "tags": "Technical-Debt, Prioritizing, Project-Management, Software-Development-Process", "date": "2018-02-05 00:00:00 +0800", "snippet": "Whether we know it or not, we probably have technical debt in our codebase! And the longer we keep it, the more problems it creates. So let’s get rid of it. But where do we start?So, what is techni...", "content": "Whether we know it or not, we probably have technical debt in our codebase! And the longer we keep it, the more problems it creates. So let’s get rid of it. But where do we start?So, what is technical debt again?Let’s have a look what Martin Fowler has to say about it: Technical Debt is a wonderful metaphor developed by Ward Cunningham to help us think about this problem. In this metaphor, doing things the quick and dirty way sets us up with a technical debt, which is similar to a financial debt. Like a financial debt, the technical debt incurs interest payments, which come in the form of the extra effort that we have to do in future development because of the quick and dirty design choice. We can choose to continue paying the interest, or we can pay down the principal by refactoring the quick and dirty design into the better design. Although it costs to pay down the principal, we gain by reduced interest payments in the future.Here is the full article for your reading pleasure.At a given point in time, it might make sense for us to create some technical debt. After all, it will give us the freedom to focus on “more important” things to do right then. We sincerely agree that we will get rid of it once there is less work to do and we have more time at hand.Making a conscious decision to add technical debt to our code base and commit to removing it later is just the first step though. The sad truth is: due to another upcoming deadline or just steadily incoming or changing requirements, we might not even think about the debt anymore.Visualising the debt we have is a necessary step that needs to follow in order to keep it on our minds so we are aware of it and can tackle it before it gets out of hand.Even though the method shown in the picture makes sure we won’t forget our technical debt, it is hard to find a volunteer.Generally, some methods have advantages over others when it comes to remembering things.Since we are all technologists, the first thing that comes to mind is storing it online, for all team members to access.But let’s face it: having a virtual space to collect debt is just a way to get it off our mind. It would be hidden on some page in a project management tool that we would never look at again.This means we need a physical board showing our debt!Now that we are aware of our technical debt it’s time to get rid of it. But how do we prioritize it?In the following we’ll be discovering two different ways of categorising and visualising technical debt that will make it easier for our team to prioritize and then decide where to focus our efforts.Technical Debt Matrix(inspired by the Eisenwhower MatrixSome types of technical debt are easy to remove and require only a small amount of effort. While other types are connected to many components of our system and require a large amount of effort to eliminate.At the same time, removing some of the technical debt offers great value to our development process and the product we are working on, while some debt removals offer nothing at all.Having a clear picture of these factors help a lot when it comes to prioritizing debt removal.In the chart above the technical debt is arranged using the two criteria mentioned above and categorised into 4 quadrants.In the future whenever we decide to create some new technical debt or discover new debt in our system, we simply take a sticky note (As an agile development team we should have them lying around everywhere), write a description of the tech debt on it, and place it on the chart according to the ease of removal and value it offers.The lower left quadrant is where removing the tech debt is expensive and only offers little value. This contains tech debt we shouldn’t focus our initial effort on.Better are the upper left and lower right quadrants. Here the removal is either easy or highly valuable, but respectively expensive or of little value.The obvious winner here is the upper right quadrant. In fact, since the top right corner in most graphs is typically the “best” quadrant, we have consciously chosen the x-axis to be ease instead of effort to have this matrix better match the common experience.Removing the technical debt located here is very valuable to our development process in addition to it also requiring only little effort to do so. Not only can we go for the low hanging fruit first but we also identified the most valuable ones. A classical win-win situation.Technical Debt TreeThe world of software development is full of metaphors to make concepts easy to grasp or just to give them a catchy name. We talk about Factory Pattern, Chaos Monkeys, Garbage Collection, we talk about how Viruses can infect our computers and how Firewalls can protect us from them. One of my favourite metaphors in software development is the Scouting Rule¹. Even Technical Debt itself is a metaphor.So let’s bring this metaphorical thinking to our tech debt visualisation.Continuing with the low hanging fruits metaphor: the sticky notes located at the lower end of the tree represent technical debt that can be solved easily; figuratively the low hanging fruits. The notes hanging higher on the tree are more difficult tasks and require more effort.Now we have the same distinction between easy and hard to remove technical debt as with the quadrant representation above. What’s missing is the visualisation of how much value it offers.Here’s where the sticky size comes into play. Just like picking a large apple from a tree gives you more nutritious value for your effort than picking a small one: The size of the notes represent the value of removing this piece of debt offers.Additionally, the different sizes of the sticky notes give an insightful overall representation on how much technical debt there really is. The bigger the area covered with sticky notes is, the more your development is slowed down by the debt you have.Independent of whether we enjoy a more analytical visualisation, or a more playful one we can now start including the removal of tech debt in our iteration planing.Summary Having a physical representation of your tech debt is important to not forget about it Categorizing the debt helps you prioritizing it correctly Focus on the technical debt which removal offers much value while at the same time does not take a lot of effort ¹: even though “Boy Scout Rule” is the more popular term I’m sure the Girl Scouts) follow the same directive." }, { "title": "IntelliJ Live Template for console.log", "url": "/intellij-live-template-for-console-log/", "categories": "", "tags": "IntelliJ, console.log(), JavaScript, TypeScript", "date": "2017-08-09 00:00:00 +0800", "snippet": "A handy shortcut for one of our most common tasksWe’ve all been there. Our code is not behaving like we hoped it would and it’s not obvious why. The easiest and fastest way to get a clue for what’s...", "content": "A handy shortcut for one of our most common tasksWe’ve all been there. Our code is not behaving like we hoped it would and it’s not obvious why. The easiest and fastest way to get a clue for what’s going on is to simply put a print statement in the code. It’s not nice; we should use a proper debugger, but we do it anyway.The following live template will add the console.log(‘’) statement to your code automatically insert the class name, function name, and line number in the log message offer code completion for the message you want to log and place the cursor where you need it.According to Larry Wall, the original author of the Perl programming language, one of the three great virtues of a programmer is laziness.So let’s automate the boring stuff!Here’s a video showing the setup and usage of the live template. You can pause and skip parts of the video or just follow the instructions below.Steps to FollowLive template preferences: Go to Preferences( ⌘ + , ). Select Editor/Live Templates (you can type live templates to find it). Select Javascript. Press the ‘+’ sign in the top right corner of the window( ⌘ + N ). Select Live Template (press 1 or Enter).Edit template: Enter the abbreviation you want to assign to the template. We will use cl in this case. Enter the description for the template (e.g. “inserts console.log(‘’);” ). Paste the following template code:console.log('Class: $CLASS$, Function: $FUNCTION$, Line $LINE$ $PARAM_TEXT$($EXPECTED$): ', $PARAM$);$END$ Click Define below the template text field and select the contexts where this template should apply (e.g. JavaScript, Typescript, etc.) Edit variables (feel free to add or leave out template variables that you (don’t) need.): Click Edit Variables. Select ‘jsClassName()’, ‘jsMethodName()’, ‘lineNumber()’ as Expression to CLASS, FUNCTION, LINE respectively. Add ‘PARAM’ as a default Value of PARAM_TEXT. Check Skip if Defined for these variables. Order variables so that PARAM comes before PARAM_TEXT and EXPECTED.Now go ahead type cl and press Enter (or Tab if that is your default). Tada! There is our template already with line number, class and function name and the cursor is waiting for you to type the second parameter which will instantly appear in the string of the log message. By pressing Enter, the cursor will jump to the next undefined parameter. Type something, press Enter again, and you are at the end of the line.And there you have your log statement!Explanation $PARAM$ is the placeholder for the expression that you want to log. $PARAM_TEXT$ will be replaced with this expression due to the default value binding. $CLASS$, $FUNCTION$, $LINE$ will be replaced with the className, methodName, and lineNumber as specified. $END$ indicates the position where the cursor will be placed after every variable has been set.The order of the variables is important since it defines the order where the cursor jumps to next after pressing Enter. Special in this case is that the cursor will skip PARAM_TEXT since it was defined through PARAM before.The mechanics used here can be applied to many other use cases like a custom logger or other general language features. Some other useful expressions that can be assigned to template variables are complete() and completeSmart(), which lets you use autocomplete within strings.I’d love to see your favourite use cases and examples in the comment section." }, { "title": "Testing Angular Components With @Input()", "url": "/angular_test_input_decorator/", "categories": "", "tags": "Angular, Test, JavaScript, TypeScript", "date": "2017-06-19 00:00:00 +0800", "snippet": "When developing an Angular (read Angular 5 or Angular 6, or whatever the current version is when you read this) component that takes an input, you might decide to unit test the whole component. At ...", "content": "When developing an Angular (read Angular 5 or Angular 6, or whatever the current version is when you read this) component that takes an input, you might decide to unit test the whole component. At least I hope you do!For example, we have a component, ComponentUnderTest, in which we want to display upcased input… I know right: what a great use-case, every business owner needs that!So, let’s say the ComponentUnderTest binds and displays input.@Component({ selector: 'component-under-test', template: '&lt;div&gt;&lt;/div&gt;'})export class ComponentUnderTestComponent{ @Input() input; processInput(): void { this.input = this.input.toUpperCase(); }}To verify that processInput() correctly upcases our input, we can simply assign a test value to the input variable and assert that, after calling the method, the displayed input is in ALL CAPS.Easy peasy! Right?But by now, we need to manually call processInput() and until we call it, our input is still displayed in lousy lowercase.Luckily, with Angular’s OnInit lifecycle hook, we can trigger our processInput() even before displaying anything. So let’s implement it and call processInput() in the corresponding method.ngOnInit(){ this.processInput();}Let’s run the tests!Oh no! The tests fail! Cannot read property ‘toUpperCase’ of undefinedAh, of course, by the time toUpperCase() is called on the input in our ngOnInit() we have not even assigned any value to it yet.When we actually embed our component in the production code, we would have set the value of input by binding the input in the component’s tag.&lt;component-under-test input=“some input”&gt;&lt;/component-under-test&gt;So let’s embed the ComponentUnderTest in a TestHostComponent. By actually having a host or parent component, we are able to pass in exactly the input we want for our test.In our test, we can simply define this TestHostComponent, include it in the testing module configuration, and instantiate it in our beforeEach method.describe('ComponentUnderTestComponent', () =&gt; { let testHostComponent: TestHostComponent; let testHostFixture: ComponentFixture&lt;TestHostComponent&gt;; beforeEach(async(() =&gt; { TestBed.configureTestingModule({ declarations: [ComponentUnderTestComponent, TestHostComponent] }) .compileComponents(); })); beforeEach(() =&gt; { testHostFixture = TestBed.createComponent(TestHostComponent); testHostComponent = testHostFixture.componentInstance; testHostFixture.detectChanges(); }); it('should show TEST INPUT', () =&gt; { expect(testHostFixture.nativeElement.querySelector('div').innerText).toEqual('TEST INPUT'); }); @Component({ selector: `host-component`, template: `&lt;component-under-test input=\"test input\"&gt;&lt;/component-under-test&gt;` }) class TestHostComponent { }});What results are green tests and the input is upcased at the beginning.However, can we really be sure that the input gets upcased and that our component not just always displays “INPUT TEXT”? Let’s write another test that upcases “different test input”.What we could do to achieve this is define another TestHostComponent which binds another input but we can do better than that!it('should show TEST INPUT', () =&gt; { testHostComponent.setInput('test input'); testHostFixture.detectChanges(); expect(testHostFixture.nativeElement.querySelector('div').innerText).toEqual('TEST INPUT');});it('should show DIFFERENT TEST INPUT', () =&gt; { testHostComponent.setInput('different test input'); testHostFixture.detectChanges(); expect(testHostFixture.nativeElement.querySelector('div').innerText).toEqual('DIFFERENT TEST INPUT');});@Component({ selector: `host-component`, template: `&lt;component-under-test [input]=\"input\"&gt;&lt;/component-under-test&gt;`})class TestHostComponent { private input: string; setInput(newInput: string) { this.input = newInput; }}setInput() sets the input in our host component. Assigning the input in our test before we let Angular detect the changes allows both our tests to pass..Now, imagine we have to bind not only one but multiple inputs to our component. Do we now need to write a setter method for each input?Luckily, the answer is no!it('should show DIFFERENT TEST INPUT', () =&gt; { testHostComponent.componentUnderTestComponent.input = 'different test input'; testHostFixture.detectChanges(); expect(testHostFixture.nativeElement.querySelector('div').innerText).toEqual('DIFFERENT TEST INPUT');});@Component({ selector: `host-component`, template: `&lt;component-under-test&gt;&lt;/component-under-test&gt;`})class TestHostComponent { @ViewChild(ComponentUnderTestComponent) public componentUnderTestComponent: ComponentUnderTestComponent;}Instead of having a setter at all, we can also get a reference to our component from within the host component and pass it into our test. There we have full control over our component and can modify any field we want.Summary To test components that bind an input via the @Input() decorator, we can create a host component in our test to wrap our test component. For multiple test inputs, we can add a setter to our host component. For more that one input binding and even more control over our component under test, we can grab it from within the host component with @ViewChild and pass it into our test directly.Complete source code can be found on GitHub.(This article has been featured in the book Angular for Enterprise-Ready Web Applications by Doguhan Uluca)" }, { "title": "[Talk] Introduction to Artificial Neural Networks and Convolutional Neural networks", "url": "/introduction-to-convolutional-neural-networks/", "categories": "", "tags": "ML, Machine-Learning, AI, Artificial-Intelligence, Talk", "date": "2016-10-04 00:00:00 +0800", "snippet": " If you follow the news in the field of technology, you will come across Artificial Neural Networks eventually, often times in relation to what was thought to be impossible just a decade ago. The...", "content": " If you follow the news in the field of technology, you will come across Artificial Neural Networks eventually, often times in relation to what was thought to be impossible just a decade ago. These networks recognize patterns in big data, beat Go champions, understand the meaning of spoken language and much more. But what do they look like and how do they actually work?! Is it black magic? (Spoiler: it’s not.) This talk will give insight into the structure of these networks and will try to explain what’s going on inside them and how they can perform all these tasks. In recent years Convolutional Neural Networks, a special type of Neural Network that is considered a deep learning network, got a lot of attention due to their remarkable results in the field of object classification and recognition. They are used in many services that we come across on a daily basis. The differences of Convolutional Neural Networks compared to general Neural Networks is explained, as well as the reasons why they perform so well when it comes to image data. This talk is an introduction to explain the basics of artificial neural networks, no previous knowledge is required. Agenda: 6.30 pm: Welcome and drinks 7.00 pm: Introduction to Artificial Neural Networks by Aiko Klostermann 8:15 pm: Q&amp;A 8:45 pm: Drinks and networkingThis talk was presented publicly at a meetup in Hamburg. There is a recording of it somewhere on some sd card in some box, I’ll upload the recording if I find it.For now: here are the slides. " }, { "title": "Privacy Policy", "url": "/privacy/", "categories": "", "tags": "", "date": "2001-01-01 00:00:00 +0800", "snippet": "Google AnalyticsThis website uses Google Analytics. This happens only if you approve third party cookies and scripts.I do this to better understand the readers of this blog so that I can create rel...", "content": "Google AnalyticsThis website uses Google Analytics. This happens only if you approve third party cookies and scripts.I do this to better understand the readers of this blog so that I can create relevant content for you." }, { "title": "Hello World", "url": "/posts/Hello-world/", "categories": "", "tags": "HelloWorld, Meta, aiko.dev", "date": "2001-01-01 00:00:00 +0800", "snippet": "Hi,I am Aiko.Software engineer, Consultant and Keynote speaker.Like many others before I now have my own website, or “Homepage” as we used to say in the 2000s.So far there’s nothing really to see, ...", "content": "Hi,I am Aiko.Software engineer, Consultant and Keynote speaker.Like many others before I now have my own website, or “Homepage” as we used to say in the 2000s.So far there’s nothing really to see, but I’ll be publishing new blog posts and other things I am working on here.I’ll also be migrating old posts and talk recordings etc. to this site.Admittedly, this is also a tiny bit about me just finally building my own website. 🤓" } ]
